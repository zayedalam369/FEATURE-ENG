{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering"
      ],
      "metadata": {
        "id": "B-9QqtvmrmXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1"
      ],
      "metadata": {
        "id": "u9cpPZIcrmUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> A parameter is a variable that is used to pass information into a function, method, or system. It acts as a placeholder that takes a specific value when the function is called."
      ],
      "metadata": {
        "id": "ClzeYb4BrmRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2"
      ],
      "metadata": {
        "id": "EvAW8LRyrmO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "-->Correlation is a statistical measure that describes the relationship between two variables. It indicates whether and how strongly the variables move together\n",
        "\n",
        "--> Negative Correlation: When one variable increases, the other decreases.\n",
        "\n",
        "Example: More exercise is often linked to lower body weight."
      ],
      "metadata": {
        "id": "UIBVUmWsrmMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3"
      ],
      "metadata": {
        "id": "zEvsn9UXrmJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Machine Learning (ML) is a branch of artificial intelligence (AI) that enables computers to learn patterns from data and make predictions or decisions without being explicitly programmed. It uses algorithms to identify trends, make classifications, or generate insights from input data.\n",
        "\n",
        "Main Components of Machine Learning\n",
        "Data\n",
        "\n",
        "The foundation of ML; includes training and test datasets.\n",
        "\n",
        "Quality and quantity of data impact model performance.\n",
        "\n",
        "Features (Input Variables)\n",
        "\n",
        "The measurable properties or characteristics used for learning.\n",
        "\n",
        "Example: In predicting house prices, features might include size, location, and number of bedrooms.\n",
        "\n",
        "Model (Algorithm)\n",
        "\n",
        "The mathematical function that learns from data and makes predictions.\n",
        "\n",
        "Examples: Decision Trees, Neural Networks, Support Vector Machines (SVM), etc.\n",
        "\n",
        "Training Process\n",
        "\n",
        "The process of feeding data into the model so it can learn relationships.\n",
        "\n",
        "Involves optimization techniques like gradient descent.\n",
        "\n",
        "Loss Function\n",
        "\n",
        "Measures how well the model's predictions match actual outcomes.\n",
        "\n",
        "Example: Mean Squared Error (MSE) for regression problems.\n",
        "\n",
        "Optimization Algorithm\n",
        "\n",
        "Adjusts model parameters to minimize the loss function.\n",
        "\n",
        "Example: Stochastic Gradient Descent (SGD).\n",
        "\n",
        "Evaluation & Validation\n",
        "\n",
        "Assessing the model‚Äôs accuracy using a separate test dataset.\n",
        "\n",
        "Common metrics: Accuracy, Precision, Recall, F1-score.\n",
        "\n",
        "Prediction/Inference\n",
        "\n",
        "Once trained, the model makes predictions on new data."
      ],
      "metadata": {
        "id": "5yjaHqjprmHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4"
      ],
      "metadata": {
        "id": "yqFVc1bVrmEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->The loss value quantifies how well or poorly a machine learning model is performing. It measures the difference between the model‚Äôs predictions and the actual values in the training or test dataset\n",
        "\n",
        "Why is Loss Important?\n",
        "Indicator of Model Performance\n",
        "\n",
        "A low loss value means the model's predictions are close to the actual values (good performance).\n",
        "\n",
        "A high loss value suggests the model is making large errors (poor performance).\n",
        "\n",
        "Guides Model Training\n",
        "\n",
        "The model updates its parameters (weights) to minimize loss using optimization algorithms like Gradient Descent.\n",
        "\n",
        "If the loss is decreasing during training, the model is learning effectively.\n",
        "\n",
        "Prevents Overfitting or Underfitting\n",
        "\n",
        "Overfitting: Very low training loss but high test loss ‚Üí Model memorized the training data but fails on new data.\n",
        "\n",
        "Underfitting: Both training and test loss are high ‚Üí Model is too simple to capture patterns"
      ],
      "metadata": {
        "id": "DHu4ONFqrmBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5"
      ],
      "metadata": {
        "id": "f9SeTp2vrl-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->1. Continuous Variables\n",
        "Represent numeric values that can take any number within a range.\n",
        "\n",
        "Can be measured (not just counted).\n",
        "\n",
        "Can have decimals or fractions.\n",
        "\n",
        "Examples:\n",
        "Height (e.g., 5.8 feet)\n",
        "\n",
        "Weight (e.g., 70.5 kg)\n",
        "\n",
        "Temperature (e.g., 36.7¬∞C)\n",
        "\n",
        "Time (e.g., 2.45 hours)\n",
        "\n",
        "üîπ Key Feature: You can perform mathematical operations like addition, subtraction, and averaging\n",
        "\n",
        "2. Categorical Variables\n",
        "Represent groups or categories rather than numerical values.\n",
        "\n",
        "Can be counted but not measured.\n",
        "\n",
        "Usually represented as labels or names (sometimes as numbers that don‚Äôt have mathematical meaning).\n",
        "\n",
        "Types of Categorical Variables:\n",
        " Nominal: No inherent order.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Colors (Red, Blue, Green)\n",
        "\n",
        "Car brands (Toyota, Ford, BMW)\n",
        "\n",
        "Gender (Male, Female, Other)\n",
        "\n",
        " Ordinal: Have a meaningful order but differences are not measurable.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Education level (High School < Bachelor's < Master's < PhD)\n",
        "\n",
        "Satisfaction level (Low, Medium, High)\n",
        "\n",
        "üîπ Key Feature: You can group and count categorical variables but cannot perform arithmetic operations on them."
      ],
      "metadata": {
        "id": "MjFmbhZhrl7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6"
      ],
      "metadata": {
        "id": "wkjly7RSrl4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->1. Encoding Techniques\n",
        "(a) One-Hot Encoding (OHE)\n",
        "Converts each category into a binary column (0 or 1).\n",
        "\n",
        "Suitable for nominal variables (no order).\n",
        "\n",
        " Example:\n",
        "\n",
        "Color\tOne-Hot Encoding\n",
        "Red\t(1,0,0)\n",
        "Blue\t(0,1,0)\n",
        "Green\t(0,0,1)\n",
        "üîπ Pros: Simple, widely used.\n",
        "üîπ Cons: Can create too many columns for high-cardinality data.\n",
        "\n",
        "(b) Label Encoding\n",
        "Assigns a unique integer to each category.\n",
        "\n",
        "Suitable for ordinal variables (where order matters).\n",
        "\n",
        " Example (Education Level):\n",
        "\n",
        "Education\tLabel Encoding\n",
        "High School\t0\n",
        "Bachelor's\t1\n",
        "Master's\t2\n",
        "PhD\t3\n",
        "üîπ Pros: Simple, uses less memory.\n",
        "üîπ Cons: Can mislead models into thinking higher numbers mean better values\n",
        "\n",
        "(c) Ordinal Encoding\n",
        "Similar to Label Encoding but assigns numbers based on a meaningful order.\n",
        "\n",
        "Works best for ordinal data (e.g., rankings, satisfaction levels).\n",
        "\n",
        " Example (Satisfaction Level):\n",
        "\n",
        "Satisfaction\tOrdinal Encoding\n",
        "Low\t0\n",
        "Medium\t1\n",
        "High\t2\n",
        "\n",
        "\n",
        "2. Frequency Encoding\n",
        "Assigns values based on category occurrence in the dataset.\n",
        "\n",
        "Works well for high-cardinality categorical features.\n",
        "\n",
        " Example (Car Brands in a dataset of 100 cars):\n",
        "\n",
        "Car Brand\tFrequency Encoding\n",
        "Toyota\t40\n",
        "Ford\t30\n",
        "BMW\t20\n",
        "Tesla\t10\n",
        "üîπ Pros: Keeps useful information, reduces feature explosion.\n",
        "üîπ Cons: May not work well if frequencies change in future data.\n",
        "\n",
        "3. Target Encoding (Mean Encoding)\n",
        "Replaces categories with the mean of the target variable (used for regression/classification).\n",
        "\n",
        " Example (Category: City, Target: Average House Price)\n",
        "\n",
        "City\tAvg House Price (Target Encoding)\n",
        "New York\t500,000\n",
        "LA\t400,000\n",
        "Chicago\t300,000\n",
        "üîπ Pros: Keeps valuable category-target relationship.\n",
        "üîπ Cons: Can lead to data leakage if not done correctly.\n",
        "\n",
        "4. Embedding (For Deep Learning)\n",
        "Uses vector representations instead of simple numbers.\n",
        "\n",
        "Especially useful for high-cardinality categorical data.\n",
        "\n",
        " Used in: Word embeddings (NLP), Recommender Systems.\n",
        "\n",
        "How to Choose the Right Technique?\n",
        "Technique\tSuitable For\tPros\tCons\n",
        "One-Hot Encoding\tNominal\tSimple, widely used\tCreates too many columns\n",
        "Label Encoding\tOrdinal\tMemory-efficient\tCan mislead models\n",
        "Ordinal Encoding\tOrdered categories\tCaptures order\tStill assumes linearity\n",
        "Frequency Encoding\tHigh-cardinality\tSimple, keeps distribution\tMight not generalize well\n",
        "Target Encoding\tSupervised learning\tCaptures category importance\tRisk of data leakage\n",
        "Embedding\tDeep Learning\tPowerful for large data\tComplex"
      ],
      "metadata": {
        "id": "zWt6jHjgrl1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7"
      ],
      "metadata": {
        "id": "TgAooifvrlyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->In machine learning, a dataset is typically split into training and testing sets to evaluate how well a model learns and generalizes to new data.\n",
        "\n",
        "1. Training Dataset\n",
        "Definition: A subset of the data used to train the model.\n",
        "\n",
        "Purpose: The model learns patterns and adjusts its parameters (weights) using this data.\n",
        "\n",
        "Size: Typically 70-80% of the total dataset.\n",
        "\n",
        " Example:\n",
        "If we have a dataset of 1,000 records, we might use 800 for training.\n",
        "\n",
        "2. Testing Dataset\n",
        "Definition: A separate subset of the data used to evaluate the model after training.\n",
        "\n",
        "Purpose: Checks how well the model performs on new, unseen data.\n",
        "\n",
        "Size: Usually 20-30% of the dataset.\n",
        "\n",
        " Example:\n",
        "If we have 1,000 records, we might use 200 for testing."
      ],
      "metadata": {
        "id": "imBu5GaKrlvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8"
      ],
      "metadata": {
        "id": "A4azEos3rlsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->sklearn.preprocessing is a module in Scikit-Learn that provides tools for scaling, transforming, and encoding data to improve model performance.\n",
        "\n",
        "Common Preprocessing Functions\n",
        "1. Scaling & Normalization (Feature Scaling)\n",
        "(a) Standardization (StandardScaler)\n",
        "Transforms data to zero mean and unit variance (Normal Distribution).\n",
        "\n",
        "Formula:\n",
        "\n",
        "ùëã\n",
        "scaled\n",
        "=\n",
        "ùëã\n",
        "‚àí\n",
        "ùúá\n",
        "ùúé\n",
        "X\n",
        "scaled\n",
        "‚Äã\n",
        " =\n",
        "œÉ\n",
        "X‚àíŒº\n",
        "‚Äã\n",
        "\n",
        " Best for: Algorithms like Logistic Regression, SVM, Neural Networks\n",
        "\n",
        " (b) Min-Max Scaling (MinMaxScaler)\n",
        "Scales data between 0 and 1.\n",
        "\n",
        "Formula:\n",
        "\n",
        "ùëã\n",
        "scaled\n",
        "=\n",
        "ùëã\n",
        "‚àí\n",
        "ùëã\n",
        "min\n",
        "ùëã\n",
        "max\n",
        "‚àí\n",
        "ùëã\n",
        "min\n",
        "X\n",
        "scaled\n",
        "‚Äã\n",
        " =\n",
        "X\n",
        "max\n",
        "‚Äã\n",
        " ‚àíX\n",
        "min\n",
        "‚Äã\n",
        "\n",
        "X‚àíX\n",
        "min\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        "\n",
        " Best for: Deep Learning, KNN, K-Means\n",
        "\n",
        " (c) Robust Scaling (RobustScaler)\n",
        "Handles outliers better by using the median and interquartile range (IQR) instead of mean.\n",
        "\n",
        "Best for: Datasets with extreme values (e.g., financial data)\n",
        "\n",
        "Why Use sklearn.preprocessing?\n",
        "Standardizes Features ‚Üí Ensures consistent value ranges (e.g., scaling numerical data).\n",
        "\n",
        "Handles Missing or Categorical Data ‚Üí Converts categories into numbers (e.g., one-hot encoding).\n",
        "\n",
        "Improves Model Accuracy ‚Üí Some models (e.g., logistic regression, neural networks) perform better with scaled inputs."
      ],
      "metadata": {
        "id": "bDSWaa69rlpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9"
      ],
      "metadata": {
        "id": "UXuByWG9rlmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->A test set is a portion of the dataset that is not used for training but is instead used to evaluate the model‚Äôs performance on new, unseen data.\n",
        "\n",
        "\n",
        "Purpose of a Test Set\n",
        "Evaluates Generalization ‚Üí Checks how well the model performs on unseen data.\n",
        "\n",
        "Prevents Overfitting ‚Üí Ensures the model does not just memorize the training data.\n",
        "\n",
        "Provides a Performance Estimate ‚Üí Helps compare models using metrics like accuracy, precision, recall, RMSE, etc."
      ],
      "metadata": {
        "id": "g1ipnDKBrljT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10"
      ],
      "metadata": {
        "id": "_1Z1C6ydrlgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->In machine learning, we split data into training and test sets to ensure our model can generalize well to new, unseen data.\n",
        "\n",
        "Using train_test_split from Scikit-Learn\n",
        "Scikit-Learn provides a simple way to split data using train_test_split.\n",
        "Key Parameters in train_test_split\n",
        "test_size=0.2 ‚Üí 20% of the data is used for testing.\n",
        "\n",
        "random_state=42 ‚Üí Ensures reproducibility (same split every time).\n",
        "\n",
        "stratify=y ‚Üí (Optional) Ensures class distribution is similar in train and test sets (useful for imbalanced datasets).\n",
        "\n",
        "Conclusion\n",
        "Splitting data is crucial for training and testing models.\n",
        "\n",
        "The ML workflow ensures a structured approach to problem-solving.\n",
        "\n",
        "Using the right preprocessing techniques improves model performance."
      ],
      "metadata": {
        "id": "tnYLg_Eprldc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset (features and labels)\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])  # Features\n",
        "y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])  # Labels\n",
        "\n",
        "# Splitting data: 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Data:\", X_train)\n",
        "print(\"Testing Data:\", X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os3fu4qEwJ7r",
        "outputId": "ba827f03-b994-4087-909e-e0c328df3084"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data: [[ 6]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [10]\n",
            " [ 5]\n",
            " [ 4]\n",
            " [ 7]]\n",
            "Testing Data: [[9]\n",
            " [2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11"
      ],
      "metadata": {
        "id": "DLtc1lY6rlar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->xploratory Data Analysis (EDA) is a critical step before training a machine learning model. It helps us understand, clean, and transform the data for better model performance.\n",
        "\n",
        "EDA improves data quality, ensures better model performance, and helps in feature selection. Skipping EDA may lead to a model that performs poorly due to noise, missing data, or irrelevant features"
      ],
      "metadata": {
        "id": "aWHTazLErlVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12"
      ],
      "metadata": {
        "id": "5DbH3A1CrlS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Correlation is a statistical measure that describes the relationship between two variables. It tells us how strongly and in what direction one variable is related to another.\n",
        "\n",
        "Types of Correlation\n",
        "Positive Correlation (+)\n",
        "\n",
        "When one variable increases, the other also increases.\n",
        "\n",
        "Example: Height vs. Weight. Taller people tend to weigh more.\n",
        "\n",
        "Negative Correlation (‚àí)\n",
        "\n",
        "When one variable increases, the other decreases.\n",
        "\n",
        "Example: Number of study hours vs. time spent on social media.\n",
        "\n",
        "No Correlation (0)\n",
        "\n",
        "No relationship between the variables.\n",
        "\n",
        "Example: Shoe size vs. Intelligence."
      ],
      "metadata": {
        "id": "3_x4tNqnrlQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13"
      ],
      "metadata": {
        "id": "OCMY-A_ArlNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->A negative correlation means that as one variable increases, the other decreases. In other words, they move in opposite directions.\n",
        "\n",
        "Example:\n",
        "\n",
        "Temperature vs. Sweater Sales ‚Üí As temperature increases, sweater sales decrease.\n",
        "\n",
        "Speed vs. Travel Time ‚Üí As speed increases, the time taken to reach a destination decreases"
      ],
      "metadata": {
        "id": "E8PrAHe1rlIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14"
      ],
      "metadata": {
        "id": "nwBahTZqrlFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Different Correlation Methods in Pandas\n",
        "Pandas supports three types of correlation:\n",
        "\n",
        "Pearson (method='pearson') ‚Üí Measures linear relationships (default).\n",
        "\n",
        "Spearman (method='spearman') ‚Üí Works well for ranked (ordinal) data.\n",
        "\n",
        "Kendall (method='kendall') ‚Üí Measures monotonic relationships."
      ],
      "metadata": {
        "id": "pOot_jX8rlCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15"
      ],
      "metadata": {
        "id": "oQX4_QIm2wxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Causation (also called cause and effect) means that one event directly causes another event to happen.\n",
        "\n",
        " If A causes B, changing A will change B.\n",
        " If A and B are correlated, it does NOT mean A causes B.\n",
        "\n",
        "\n",
        "Difference Between Correlation and Causation\n",
        "Aspect\tCorrelation\tCausation\n",
        "Definition\tA relationship where two variables move together\tOne variable directly influences another\n",
        "Direction\tNo clear cause-effect\tA causes B\n",
        "Example\tIce cream sales ‚¨Ü & Drowning cases ‚¨Ü\tEating contaminated food ‚Üí Food poisoning\n",
        "Mathematical Measure\tPearson‚Äôs Correlation Coefficient (r)\tControlled experiments & causal inference\n",
        "\n",
        "Example: Correlation vs. Causation\n",
        "Example 1: Ice Cream Sales & Drowning\n",
        "Observation: More ice cream sales ‚Üí More drowning cases.\n",
        "\n",
        "Correlation:  Strong positive correlation.\n",
        "\n",
        "Causation:  Ice cream does NOT cause drowning!\n",
        "\n",
        "Real Cause: Hot weather increases both ice cream sales and swimming.\n",
        "\n",
        "Example 2: Smoking & Lung Cancer\n",
        "Observation: More smoking ‚Üí More lung cancer cases.\n",
        "\n",
        "Correlation:  Yes, strong correlation.\n",
        "\n",
        "Causation:  Yes, because research shows smoking damages lungs.\n",
        "\n"
      ],
      "metadata": {
        "id": "cVdp0whDsAxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16"
      ],
      "metadata": {
        "id": "buTY3FEJ3Mmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->An optimizer is an algorithm that adjusts the parameters (weights and biases) of a machine learning model to minimize the loss function and improve accuracy.\n",
        "\n",
        "\n",
        "Types of Optimizers in Machine Learning\n",
        "There are two main categories of optimizers:\n",
        "\n",
        "First-Order Optimizers (based on gradients)\n",
        "\n",
        "Second-Order Optimizers (use second derivatives; computationally expensive)\n",
        "\n",
        "Most deep learning models use first-order optimizers, such as:\n",
        "\n",
        "Gradient Descent (GD)\n",
        "\n",
        "Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Momentum-based optimizers (Momentum, NAG)\n",
        "\n",
        "Adaptive optimizers (Adam, RMSprop, Adagrad, Adadelta)\n",
        "\n",
        " Gradient Descent (GD)\n",
        "Gradient Descent updates model parameters by moving in the direction of the negative gradient of the loss function.\n",
        "\n",
        "Formula:\n",
        "ùëä\n",
        "=\n",
        "ùëä\n",
        "‚àí\n",
        "ùõº\n",
        "‚ãÖ\n",
        "‚àÇ\n",
        "ùêø\n",
        "‚àÇ\n",
        "ùëä\n",
        "W=W‚àíŒ±‚ãÖ\n",
        "‚àÇW\n",
        "‚àÇL\n",
        "‚Äã\n",
        "\n",
        "where:\n",
        "\n",
        "ùëä\n",
        "W = model parameters (weights)\n",
        "\n",
        "ùõº\n",
        "Œ± = learning rate\n",
        "\n",
        "‚àÇ\n",
        "ùêø\n",
        "‚àÇ\n",
        "ùëä\n",
        "‚àÇW\n",
        "‚àÇL\n",
        "‚Äã\n",
        "  = gradient of loss\n",
        "\n",
        "Variants of Gradient Descent:\n",
        "Type\tDescription\n",
        "Batch GD\tComputes gradient on the entire dataset (slow but stable).\n",
        "Stochastic GD (SGD)\tUpdates weights after each sample (faster but noisy).\n",
        "Mini-Batch GD\tUpdates weights using small batches (balances speed & stability).\n",
        "\n",
        " Stochastic Gradient Descent (SGD)\n",
        "Instead of computing gradients on the entire dataset, SGD updates weights after each training example.\n",
        " Faster but noisier updates.\n",
        "\n",
        " Momentum Optimizer\n",
        "Momentum adds a velocity term to accelerate SGD in relevant directions and dampen oscillations.\n",
        "‚úÖ Solves slow convergence in SGD.\n",
        "\n",
        "Formula:\n",
        "ùë£\n",
        "ùë°\n",
        "=\n",
        "ùõΩ\n",
        "ùë£\n",
        "ùë°\n",
        "‚àí\n",
        "1\n",
        "+\n",
        "ùõº\n",
        "‚àá\n",
        "ùêø\n",
        "v\n",
        "t\n",
        "‚Äã\n",
        " =Œ≤v\n",
        "t‚àí1\n",
        "‚Äã\n",
        " +Œ±‚àáL\n",
        "ùëä\n",
        "=\n",
        "ùëä\n",
        "‚àí\n",
        "ùë£\n",
        "ùë°\n",
        "W=W‚àív\n",
        "t\n",
        "‚Äã\n",
        "\n",
        "where:\n",
        "\n",
        "ùë£\n",
        "ùë°\n",
        "v\n",
        "t\n",
        "‚Äã\n",
        "  = velocity\n",
        "\n",
        "ùõΩ\n",
        "Œ≤ = momentum coefficient (e.g., 0.9)\n",
        "\n",
        "Nesterov Accelerated Gradient (NAG)\n",
        "NAG is an improvement over Momentum that looks ahead before computing the gradient.\n",
        "‚úÖ Prevents overshooting and improves convergence speed.\n",
        "\n",
        "Formula:\n",
        "ùë£\n",
        "ùë°\n",
        "=\n",
        "ùõΩ\n",
        "ùë£\n",
        "ùë°\n",
        "‚àí\n",
        "1\n",
        "+\n",
        "ùõº\n",
        "‚àá\n",
        "ùêø\n",
        "(\n",
        "ùëä\n",
        "‚àí\n",
        "ùõΩ\n",
        "ùë£\n",
        "ùë°\n",
        "‚àí\n",
        "1\n",
        ")\n",
        "v\n",
        "t\n",
        "‚Äã\n",
        " =Œ≤v\n",
        "t‚àí1\n",
        "‚Äã\n",
        " +Œ±‚àáL(W‚àíŒ≤v\n",
        "t‚àí1\n",
        "‚Äã\n",
        " )\n",
        "ùëä\n",
        "=\n",
        "ùëä\n",
        "‚àí\n",
        "ùë£\n",
        "ùë°\n",
        "W=W‚àív\n",
        "t\n",
        "‚Äã\n",
        "\n",
        "\n",
        "daptive Optimizers\n",
        "Adaptive optimizers adjust the learning rate dynamically for each parameter.\n",
        "\n",
        "5.1 Adagrad (Adaptive Gradient Algorithm)\n",
        "Adapts learning rate for each parameter.\n",
        "\n",
        "Problem: Learning rate keeps decreasing, making training slow.\n",
        "\n",
        "5.2 RMSprop (Root Mean Square Propagation)\n",
        "Solves Adagrad‚Äôs decaying learning rate issue using a moving average.\n",
        " Works well for deep learning tasks (e.g., RNNs).\n",
        "\n",
        "5.3 Adam (Adaptive Moment Estimation)\n",
        "Adam combines Momentum + RMSprop, making it the most widely used optimizer.\n",
        "‚úÖ Works well across many deep learning tasks.\n",
        "\n",
        "Formula:\n",
        "ùëö\n",
        "ùë°\n",
        "=\n",
        "ùõΩ\n",
        "1\n",
        "ùëö\n",
        "ùë°\n",
        "‚àí\n",
        "1\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùõΩ\n",
        "1\n",
        ")\n",
        "‚àá\n",
        "ùêø\n",
        "m\n",
        "t\n",
        "‚Äã\n",
        " =Œ≤\n",
        "1\n",
        "‚Äã\n",
        " m\n",
        "t‚àí1\n",
        "‚Äã\n",
        " +(1‚àíŒ≤\n",
        "1\n",
        "‚Äã\n",
        " )‚àáL\n",
        "ùë£\n",
        "ùë°\n",
        "=\n",
        "ùõΩ\n",
        "2\n",
        "ùë£\n",
        "ùë°\n",
        "‚àí\n",
        "1\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùõΩ\n",
        "2\n",
        ")\n",
        "(\n",
        "‚àá\n",
        "ùêø\n",
        ")\n",
        "2\n",
        "v\n",
        "t\n",
        "‚Äã\n",
        " =Œ≤\n",
        "2\n",
        "‚Äã\n",
        " v\n",
        "t‚àí1\n",
        "‚Äã\n",
        " +(1‚àíŒ≤\n",
        "2\n",
        "‚Äã\n",
        " )(‚àáL)\n",
        "2\n",
        "\n",
        "ùëä\n",
        "=\n",
        "ùëä\n",
        "‚àí\n",
        "ùõº\n",
        "‚ãÖ\n",
        "ùëö\n",
        "ùë°\n",
        "ùë£\n",
        "ùë°\n",
        "+\n",
        "ùúñ\n",
        "W=W‚àí\n",
        "v\n",
        "t\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        " +œµ\n",
        "Œ±‚ãÖm\n",
        "t\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        "\n"
      ],
      "metadata": {
        "id": "dkij5OS5sAvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17"
      ],
      "metadata": {
        "id": "4k8QYMpisAsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->sklearn.linear_model in Scikit-Learn\n",
        "sklearn.linear_model is a module in Scikit-Learn that provides different linear models for regression and classification tasks. These models assume a linear relationship between input features and the target variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "S5Nwr8x6sAp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18"
      ],
      "metadata": {
        "id": "Cu0CQR38sAnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->The .fit() method in machine learning trains a model by learning from the given dataset.\n",
        "- It finds the best model parameters (weights & biases) to minimize loss.\n",
        "- It adjusts the model based on training data.\n",
        "\n",
        "Takes Input Data (X) and Target Labels (y).\n",
        "\n",
        "Computes Loss ‚Üí Measures how far predictions are from actual values.\n",
        "\n",
        "Optimizes Model Parameters (weights & biases) using an optimizer (e.g., Gradient Descent).\n",
        "\n",
        "Repeats for multiple epochs until convergence."
      ],
      "metadata": {
        "id": "7IJNk9DCsAkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19"
      ],
      "metadata": {
        "id": "6dGlzSgOsAhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->model.predict() is used to make predictions after a machine learning model has been trained using fit().\n",
        "\n",
        "- It takes input data (X_new) and returns the predicted values (y_pred).\n",
        "- It does NOT update model parameters‚Äîit only performs inference.\n",
        "\n",
        "Takes new input data (X_new).\n",
        "\n",
        "Uses the trained model to calculate the output.\n",
        "\n",
        "Returns predicted values (y_pred)."
      ],
      "metadata": {
        "id": "1lszp5VMsAe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20"
      ],
      "metadata": {
        "id": "nqjunRRysAb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Continuous Variables\n",
        "A continuous variable can take any numeric value within a range.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Height (cm) ‚Üí 165.5 cm, 170.2 cm\n",
        "\n",
        "Weight (kg) ‚Üí 55.3 kg, 60.8 kg\n",
        "\n",
        "Temperature (¬∞C) ‚Üí 36.5¬∞C, 98.6¬∞F\n",
        "\n",
        "Salary ($) ‚Üí $40,000, $55,500\n",
        "\n",
        " Key Features:\n",
        "\n",
        "Measured on a scale (e.g., meters, kilograms, dollars).\n",
        "\n",
        "Can have decimal values (fractions).\n",
        "\n",
        "Uses mathematical operations like addition & multiplication.\n",
        "\n",
        "Example Dataset (Continuous Variables)\n",
        "Person\tAge (Years)\tSalary ($)\tHeight (cm)\n",
        "Alice\t25\t50,000\t165.2\n",
        "Bob\t30\t60,000\t175.8\n",
        "\n",
        " Categorical Variables\n",
        "A categorical variable represents labels or categories and does NOT have a numerical meaning.\n",
        "\n",
        " Types of Categorical Variables:\n",
        "\n",
        "Nominal (No order)\n",
        "\n",
        "Example: Gender (Male, Female, Other)\n",
        "\n",
        "Example: Car Brand (Toyota, BMW, Tesla)\n",
        "\n",
        "Ordinal (Ordered categories)\n",
        "\n",
        "Example: Education Level (High School < Bachelor < Master < PhD)\n",
        "\n",
        "Example: Customer Satisfaction (Low, Medium, High)\n",
        "\n",
        " Key Features:\n",
        "\n",
        "Represents labels or groups.\n",
        "\n",
        "Cannot perform arithmetic operations (e.g., \"Red\" + \"Blue\" doesn‚Äôt make sense).\n",
        "\n",
        "Ordinal variables have a meaningful order, but the difference between values is not measurable.\n",
        "\n",
        "Example Dataset (Categorical Variables)\n",
        "Person\tGender\tEducation Level\n",
        "Alice\tFemale\tBachelor's\n",
        "Bob\tMale\tMaster's"
      ],
      "metadata": {
        "id": "BoFm41X4sAY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21"
      ],
      "metadata": {
        "id": "hXQvwSVwsAWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Feature Scaling is the process of transforming numerical features so that they have a similar scale (range of values). It helps machine learning models perform better by ensuring that features are comparable.\n",
        "\n",
        " Why? Some machine learning models (e.g., Gradient Descent, KNN, SVM) are sensitive to feature magnitudes. Scaling ensures that one large-value feature (e.g., Salary in $$) doesn‚Äôt dominate a small-value feature (e.g., Age in years).\n",
        "\n",
        " How Does Feature Scaling Help?\n",
        "Improves Model Performance\n",
        "\n",
        "Prevents some features from dominating others.\n",
        "\n",
        "Helps gradient-based models (like Neural Networks) converge faster.\n",
        "\n",
        "Avoids Numerical Instability\n",
        "\n",
        "Prevents overflow/underflow issues in calculations.\n",
        "\n",
        "Essential for Distance-Based Algorithms\n",
        "\n",
        "K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and K-Means clustering rely on distance calculations (e.g., Euclidean Distance).\n",
        "\n",
        "Unscaled features can lead to biased distance measures."
      ],
      "metadata": {
        "id": "MQ5vdVWIsASy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22"
      ],
      "metadata": {
        "id": "qyGjlxWasAPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Min-Max Scaling (Normalization)\n",
        " Scales features between 0 and 1\n",
        " Best for Neural Networks & Image Processing\n",
        " Not good for data with outliers\n",
        "\n",
        " Standardization (Z-Score Normalization)\n",
        " Transforms data to have mean = 0 and standard deviation = 1\n",
        " Best for SVM, KNN, PCA\n",
        " Not ideal for data with extreme outliers\n",
        "\n",
        " Robust Scaling (Handles Outliers)\n",
        " Uses median and IQR (Interquartile Range) for scaling\n",
        " Best for datasets with outliers\n",
        " Less effective when data is normally distributed\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "-nAkPnDYsAMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23"
      ],
      "metadata": {
        "id": "yeAk1oZWsAJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->sklearn.preprocessing is a module in Scikit-Learn that provides feature transformation tools to prepare data for machine learning models.\n",
        "\n",
        " Why? Many ML algorithms require data to be:\n",
        "\n",
        "Scaled (e.g., Standardization, Min-Max Scaling)\n",
        "\n",
        "Encoded (e.g., One-Hot Encoding for categorical data)\n",
        "\n",
        "Transformed (e.g., Polynomial Features, Binarization)\n",
        "\n"
      ],
      "metadata": {
        "id": "tihhNd-GsKOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24"
      ],
      "metadata": {
        "id": "lpDlfFBgsKLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->In machine learning, we split data into:\n",
        "\n",
        "Training Set ‚Üí Used to train the model.\n",
        "\n",
        "Testing Set ‚Üí Used to evaluate the model's performance on unseen data\n",
        "\n",
        "Using train_test_split from Scikit-Learn\n",
        "The train_test_split function from sklearn.model_selection randomly splits data into training and testing sets.\n",
        "\n",
        "Always split data before training!\n",
        "\n",
        "Use train_test_split() for easy splitting.\n",
        "\n",
        "Stratify when working with classification problems to maintain class balance.\n",
        "\n",
        "Consider Train-Validation-Test split when tuning hyperparameters."
      ],
      "metadata": {
        "id": "MgstoFIAsKJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25"
      ],
      "metadata": {
        "id": "fhTOnhZRsKGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Data encoding is the process of converting categorical (non-numeric) data into a format that machine learning models can understand. Since most ML algorithms only work with numerical values, categorical variables must be encoded properly\n",
        "\n",
        "Types of Data Encoding in ML\n",
        "1Ô∏è Label Encoding\n",
        "Converts categories into numerical labels (e.g., \"Male\" ‚Üí 0, \"Female\" ‚Üí 1).\n",
        "\n",
        "Best for ordinal data (e.g., \"Low\" ‚Üí 0, \"Medium\" ‚Üí 1, \"High\" ‚Üí 2).\n",
        "\n",
        " Issue: Can introduce unintended relationships (e.g., \"Red\" ‚Üí 1, \"Blue\" ‚Üí 2 might imply \"Blue\" is greater than \"Red\").\n",
        "\n",
        " One-Hot Encoding (OHE)\n",
        "Converts categories into binary columns (0s & 1s).\n",
        "\n",
        "Best for nominal (non-ordered) data.\n",
        "\n",
        " Issue: Can create many columns when categories are numerous\n",
        " Ordinal Encoding\n",
        "Assigns ranked numerical values (e.g., \"Small\" ‚Üí 0, \"Medium\" ‚Üí 1, \"Large\" ‚Üí 2).\n",
        "\n",
        "Best for ordinal (ordered) data.\n",
        "\n",
        "Issue: Should not be used for nominal data (like colors, names)."
      ],
      "metadata": {
        "id": "Y6PoYiHRsKEC"
      }
    }
  ]
}